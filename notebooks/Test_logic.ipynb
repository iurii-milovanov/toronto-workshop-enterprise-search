{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.models import QueryType\n",
    "from azure.search.documents.models import Vector\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "import openai\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Cognitive Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_SEARCH_SERVICE = os.getenv(\"AZURE_SEARCH_SERVICE\", \"ai-assistant-search\")\n",
    "AZURE_SEARCH_INDEX = os.getenv(\"AZURE_SEARCH_INDEX\", \"ai-assistant-idx\")\n",
    "COGNITIVE_SEARCH_API_KEY = os.getenv(\n",
    "    \"COGNITIVE_SEARCH_API_KEY\",\n",
    "    \"9qZ6DSjDa7TzbwwUmuCVu9D2AwuEDiT61O2vMwsTfYAzSeAx9dzL\",\n",
    ")\n",
    "\n",
    "KB_FIELDS_CONTENT = os.getenv(\"KB_FIELDS_CONTENT\", \"Content\")\n",
    "KB_FIELDS_CATEGORY = os.getenv(\"KB_FIELDS_CATEGORY\", \"Storage\")\n",
    "KB_FIELDS_SOURCEPAGE = os.getenv(\"KB_FIELDS_SOURCEPAGE\", \"LocationURL\")\n",
    "\n",
    "sourcepage_field = KB_FIELDS_SOURCEPAGE\n",
    "content_field = KB_FIELDS_CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_cognitive_search_embedding_key_credential = AzureKeyCredential(\n",
    "    COGNITIVE_SEARCH_API_KEY\n",
    ")\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{AZURE_SEARCH_SERVICE}.search.windows.net/\",\n",
    "    index_name=AZURE_SEARCH_INDEX,\n",
    "    credential=azure_cognitive_search_embedding_key_credential,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_SERVICE = os.getenv(\n",
    "    \"AZURE_OPENAI_SERVICE\", \"ai-assistant-openai1\"\n",
    ")\n",
    "OPENAI_API_KEY = os.getenv(\n",
    "    \"OPENAI_API_KEY\", \"22f8d7ccbb6b4ea39073d16155e310a6\"\n",
    ")\n",
    "API_VERSION = \"2023-05-15\"\n",
    "OPENAI_API_TYPE = \"azure\"\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = OPENAI_API_TYPE\n",
    "os.environ[\n",
    "    \"OPENAI_API_BASE\"\n",
    "] = f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_VERSION\"] = API_VERSION\n",
    "\n",
    "openai.api_type = OPENAI_API_TYPE\n",
    "openai.api_base = f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "openai.api_version = API_VERSION\n",
    "\n",
    "AZURE_OPENAI_CHATGPT_DEPLOYMENT = os.getenv(\n",
    "    \"AZURE_OPENAI_CHATGPT_DEPLOYMENT\", \"ai-assistant-gpt-35-16k\"\n",
    ")\n",
    "AZURE_OPENAI_CHATGPT_MODEL = os.getenv(\n",
    "    \"AZURE_OPENAI_CHATGPT_MODEL\", \"gpt-35-turbo-16k\"\n",
    ")\n",
    "AZURE_OPENAI_GPT4_DEPLOYMENT = os.getenv(\n",
    "    \"AZURE_OPENAI_GPT4_DEPLOYMENT\", \"ai-assistant-gpt-4\"\n",
    ")\n",
    "AZURE_OPENAI_GPT4_MODEL = os.getenv(\n",
    "    \"AZURE_OPENAI_GPT4_MODEL\",\n",
    "    \"gpt-4-32k\",\n",
    ")\n",
    "AZURE_OPENAI_EMB_DEPLOYMENT = os.getenv(\n",
    "    \"AZURE_OPENAI_EMB_DEPLOYMENT\", \"ai-assistant-ada\"\n",
    ")\n",
    "\n",
    "llm_gpt35 = AzureChatOpenAI(\n",
    "    deployment_name=AZURE_OPENAI_CHATGPT_DEPLOYMENT,\n",
    "    model_name=AZURE_OPENAI_CHATGPT_MODEL,\n",
    ")\n",
    "llm_gpt4 = AzureChatOpenAI(\n",
    "    deployment_name=AZURE_OPENAI_GPT4_DEPLOYMENT,\n",
    "    model_name=AZURE_OPENAI_GPT4_MODEL,\n",
    ")\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(deployment=AZURE_OPENAI_EMB_DEPLOYMENT)\n",
    "\n",
    "chatgpt_deployment = AZURE_OPENAI_CHATGPT_DEPLOYMENT\n",
    "chatgpt_model = AZURE_OPENAI_CHATGPT_MODEL\n",
    "gpt4_deployment = AZURE_OPENAI_GPT4_DEPLOYMENT\n",
    "gpt4_model = AZURE_OPENAI_GPT4_MODEL\n",
    "embedding_deployment = AZURE_OPENAI_EMB_DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embed_query(query_text):\n",
    "    return embeddings_model.embed_query(query_text)\n",
    "\n",
    "\n",
    "query_vector = embed_query(\"retail\")\n",
    "len(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion = openai.ChatCompletion.create(\n",
    "    deployment_id=AZURE_OPENAI_GPT4_DEPLOYMENT,\n",
    "    model=AZURE_OPENAI_GPT4_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"},\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    ")\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "SYSTEM = \"system\"\n",
    "USER = \"user\"\n",
    "ASSISTANT = \"assistant\"\n",
    "\n",
    "MODELS_2_TOKEN_LIMITS = {\n",
    "    \"gpt-35-turbo\": 4000,\n",
    "    \"gpt-3.5-turbo\": 4000,\n",
    "    \"gpt-35-turbo-16k\": 16000,\n",
    "    \"gpt-3.5-turbo-16k\": 16000,\n",
    "    \"gpt-4\": 8100,\n",
    "    \"gpt-4-32k\": 32000,\n",
    "}\n",
    "\n",
    "AOAI_2_OAI = {\n",
    "    \"gpt-35-turbo\": \"gpt-3.5-turbo\",\n",
    "    \"gpt-35-turbo-16k\": \"gpt-3.5-turbo-16k\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_token_limit(model_id: str) -> int:\n",
    "    if model_id not in MODELS_2_TOKEN_LIMITS:\n",
    "        raise ValueError(\"Expected model gpt-35-turbo and above\")\n",
    "    return MODELS_2_TOKEN_LIMITS[model_id]\n",
    "\n",
    "\n",
    "class MessageBuilder:\n",
    "    \"\"\"\n",
    "    A class for building and managing messages in a chat conversation.\n",
    "    Attributes:\n",
    "        message (list): A list of dictionaries representing chat messages.\n",
    "        model (str): The name of the ChatGPT model.\n",
    "        token_count (int): The total number of tokens in the conversation.\n",
    "    Methods:\n",
    "        __init__(self, system_content: str, chatgpt_model: str): Initializes the MessageBuilder instance.\n",
    "        append_message(self, role: str, content: str, index: int = 1): Appends a new message to the conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_content: str, chatgpt_model: str):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_content}]\n",
    "        self.model = chatgpt_model\n",
    "        self.token_length = num_tokens_from_messages(\n",
    "            self.messages[-1], self.model\n",
    "        )\n",
    "\n",
    "    def append_message(self, role: str, content: str, index: int = 1):\n",
    "        self.messages.insert(index, {\"role\": role, \"content\": content})\n",
    "        self.token_length += num_tokens_from_messages(\n",
    "            self.messages[index], self.model\n",
    "        )\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(message: dict[str, str], model: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the number of tokens required to encode a message.\n",
    "    Args:\n",
    "        message (dict): The message to encode, represented as a dictionary.\n",
    "        model (str): The name of the model to use for encoding.\n",
    "    Returns:\n",
    "        int: The total number of tokens required to encode the message.\n",
    "    Example:\n",
    "        message = {'role': 'user', 'content': 'Hello, how are you?'}\n",
    "        model = 'gpt-3.5-turbo'\n",
    "        num_tokens_from_messages(message, model)\n",
    "        output: 11\n",
    "    \"\"\"\n",
    "    # encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-16k\")\n",
    "    encoding = tiktoken.encoding_for_model(get_oai_chatmodel_tiktok(model))\n",
    "    num_tokens = 2  # For \"role\" and \"content\" keys\n",
    "    for key, value in message.items():\n",
    "        num_tokens += len(encoding.encode(value))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def get_oai_chatmodel_tiktok(aoaimodel: str) -> str:\n",
    "    message = \"Expected Azure OpenAI ChatGPT model name\"\n",
    "    if aoaimodel == \"\" or aoaimodel is None:\n",
    "        raise ValueError(message)\n",
    "    if aoaimodel not in AOAI_2_OAI and aoaimodel not in MODELS_2_TOKEN_LIMITS:\n",
    "        raise ValueError(message)\n",
    "    return AOAI_2_OAI.get(aoaimodel) or aoaimodel\n",
    "\n",
    "\n",
    "def get_messages_from_history(\n",
    "    system_prompt: str,\n",
    "    model_id: str,\n",
    "    history: list[dict[str, str]],\n",
    "    user_conv: str,\n",
    "    few_shots=[],\n",
    "    max_tokens: int = 4096,\n",
    ") -> list:\n",
    "    message_builder = MessageBuilder(system_prompt, model_id)\n",
    "\n",
    "    # Add examples to show the chat what responses we want. It will try to mimic any responses and make sure they match the rules laid out in the system message.\n",
    "    for shot in few_shots:\n",
    "        message_builder.append_message(shot.get(\"role\"), shot.get(\"content\"))\n",
    "\n",
    "    user_content = user_conv\n",
    "    append_index = len(few_shots) + 1\n",
    "\n",
    "    message_builder.append_message(USER, user_content, index=append_index)\n",
    "\n",
    "    for h in reversed(history[:-1]):\n",
    "        if bot_msg := h.get(\"bot\"):\n",
    "            message_builder.append_message(\n",
    "                ASSISTANT, bot_msg, index=append_index\n",
    "            )\n",
    "        if user_msg := h.get(\"user\"):\n",
    "            message_builder.append_message(USER, user_msg, index=append_index)\n",
    "        if message_builder.token_length > max_tokens:\n",
    "            break\n",
    "\n",
    "    messages = message_builder.messages\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_token_limit = get_token_limit(chatgpt_model)\n",
    "gpt4_token_limit = get_token_limit(gpt4_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Hello, tell me about our case studies around blockchain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [{\"user\": user_input}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query_prompt = \"\"\"Below is a history of the conversation so far, and a new question asked by the user that needs to be answered by searching in a corporate knowledge base.\n",
    "Generate a search query based on the conversation and the new question.\n",
    "Do not include cited source filenames and document names e.g info.txt or doc.pdf in the search query terms.\n",
    "Do no include any search operators like \"site:\" in the search query terms.\n",
    "Do not include any text inside [] or <<>> in the search query terms.\n",
    "Do not include any special characters like '+'.\n",
    "If the question is not in English, translate the question to English before generating the search query.\n",
    "If you cannot generate a search query, return just the number 0.\n",
    "\"\"\"\n",
    "\n",
    "search_query_prompt_few_shots = [\n",
    "    {\n",
    "        \"role\": USER,\n",
    "        \"content\": \"What projects did SoftServe deliver in the retail industry?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": ASSISTANT,\n",
    "        \"content\": \"Show available case studies in the retail industry\",\n",
    "    },\n",
    "    {\"role\": USER, \"content\": \"What AI services does SoftServe offer?\"},\n",
    "    {\n",
    "        \"role\": ASSISTANT,\n",
    "        \"content\": \"Describe AI services SoftServe provides\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_user_question = history[-1][\"user\"]\n",
    "\n",
    "search_query_messages = get_messages_from_history(\n",
    "    search_query_prompt,\n",
    "    chatgpt_model,\n",
    "    history,\n",
    "    \"Generate search query for: \" + original_user_question,\n",
    "    search_query_prompt_few_shots,\n",
    "    max_tokens=chatgpt_token_limit - len(search_query_prompt),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SoftServe blockchain case studies'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_query_completion = openai.ChatCompletion.create(\n",
    "    deployment_id=chatgpt_deployment,\n",
    "    model=chatgpt_model,\n",
    "    messages=search_query_messages,\n",
    "    temperature=0.0,\n",
    "    max_tokens=32,\n",
    "    n=1,\n",
    ")\n",
    "search_query = search_query_completion.choices[0].message.content\n",
    "search_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_clf_prompt = \"\"\"\n",
    "You're an assistant for SoftServe employees, answering their corporate inquiries.\n",
    "You have access to the following data sources:\n",
    "1. SoftServe Website - contains SoftServe's case studies, whitepapers, and other marketing materials.\n",
    "2. SoftServe Wikipedia page - contains general information about SoftServe, including history and leadership team.\n",
    "Based on the User Question, classify the data sources that are most likely to contain the answer. Return the numbers of the data sources separated by commas. If you cannot classify the data sources, return just the number 0. Do not include any text before or after the numbers. If you need to return multiple numbers, return them in ascending order, separated by commas. For example, if you need to return 1 and 2, return \"1,2\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '\\nYou\\'re an assistant for SoftServe employees, answering their corporate inquiries.\\nYou have access to the following data sources:\\n1. SoftServe Website - contains SoftServe\\'s case studies, whitepapers, and other marketing materials.\\n2. SoftServe Wikipedia page - contains general information about SoftServe, including history and leadership team.\\nBased on the User Question, classify the data sources that are most likely to contain the answer. Return the numbers of the data sources separated by commas. If you cannot classify the data sources, return just the number 0. Do not include any text before or after the numbers. If you need to return multiple numbers, return them in ascending order, separated by commas. For example, if you need to return 1 and 2, return \"1,2\".\\n'},\n",
       " {'role': 'user',\n",
       "  'content': 'User Question: SoftServe blockchain case studies'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_clf_messages = get_messages_from_history(\n",
    "    source_clf_prompt,\n",
    "    chatgpt_model,\n",
    "    [],\n",
    "    \"User Question: \" + search_query,\n",
    "    max_tokens=chatgpt_token_limit - len(source_clf_prompt),\n",
    ")\n",
    "source_clf_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SoftServe Website']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_data_source = {\n",
    "    \"1\": \"SoftServe Website\",\n",
    "    \"2\": \"Wikipedia\",\n",
    "    \"0\": \"Unknown\",\n",
    "}\n",
    "\n",
    "source_clf_completion = await openai.ChatCompletion.acreate(\n",
    "    deployment_id=chatgpt_deployment,\n",
    "    model=chatgpt_model,\n",
    "    messages=source_clf_messages,\n",
    "    temperature=0.0,\n",
    "    max_tokens=32,\n",
    "    n=1,\n",
    ")\n",
    "data_source_ids = source_clf_completion.choices[0].message.content\n",
    "data_sources = [\n",
    "    id_to_data_source.get(data_source_id.strip(), \"Unknown\")\n",
    "    for data_source_id in data_source_ids.split(\",\")\n",
    "]\n",
    "data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_idx(query_text, data_sources, top=5):\n",
    "    def nonewlines(s: str) -> str:\n",
    "        return s.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n",
    "    query_vector = embed_query(query_text)\n",
    "    title_embedding = Vector(\n",
    "        value=query_vector, k=top, fields=\"title_embedding\"\n",
    "    )\n",
    "    content_embedding = Vector(\n",
    "        value=query_vector, k=top, fields=\"content_embedding\"\n",
    "    )\n",
    "    summary_embedding = Vector(\n",
    "        value=query_vector, k=top, fields=\"summary_embedding\"\n",
    "    )\n",
    "\n",
    "    filter = (\n",
    "        f\"search.in(Storage, '{','.join(data_sources)}', ',')\"\n",
    "        if \"Unknown\" not in data_sources\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    r = search_client.search(\n",
    "        query_text,\n",
    "        filter=filter,\n",
    "        query_type=QueryType.SEMANTIC,\n",
    "        query_language=\"en-us\",\n",
    "        query_speller=\"lexicon\",\n",
    "        semantic_configuration_name=\"default\",\n",
    "        top=top,\n",
    "        query_caption=None,\n",
    "        vectors=[title_embedding, content_embedding, summary_embedding],\n",
    "    )\n",
    "\n",
    "    results_formatted = [\n",
    "        {\n",
    "            \"title\": doc[\"FileName\"],\n",
    "            \"content\": nonewlines(doc[content_field]),\n",
    "            \"summary\": nonewlines(doc[\"Summary\"]),\n",
    "            \"url\": doc[sourcepage_field],\n",
    "            \"storage\": doc[\"Storage\"],\n",
    "        }\n",
    "        for doc in r\n",
    "    ]\n",
    "    results = [res[\"url\"] + \": \" + res[\"content\"] for res in results_formatted]\n",
    "    content = \"\\n\".join(results)\n",
    "    return results_formatted, results, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SoftServe Website: building-a-blockchain-based-exchange-platform-invoice-trading-case.pdf: https://www.softserveinc.com/files/cases/building-a-blockchain-based-exchange-platform-invoice-trading-case.pdf',\n",
       " 'SoftServe Website: building-a-blockchain-based-exchange-platform-invoice-trading-case.pdf: https://www.softserveinc.com/files/cases/building-a-blockchain-based-exchange-platform-invoice-trading-case.pdf',\n",
       " 'SoftServe Website: merging-iot-blockchain-smart-report-system.pdf: https://www.softserveinc.com/files/cases/merging-iot-blockchain-smart-report-system.pdf',\n",
       " 'SoftServe Website: building-a-blockchain-based-exchange-platform-invoice-trading-case.pdf: https://www.softserveinc.com/files/cases/building-a-blockchain-based-exchange-platform-invoice-trading-case.pdf',\n",
       " 'SoftServe Website: merging-iot-blockchain-smart-report-system.pdf: https://www.softserveinc.com/files/cases/merging-iot-blockchain-smart-report-system.pdf',\n",
       " 'SoftServe Website: leading-coin-offering-asian-fintech-startup.pdf: https://www.softserveinc.com/files/cases/leading-coin-offering-asian-fintech-startup.pdf',\n",
       " 'SoftServe Website: dlt-2-0-is-all-about-scalability-whitepaper.pdf: https://www.softserveinc.com/files/whitepaper/dlt-2-0-is-all-about-scalability-whitepaper.pdf',\n",
       " 'SoftServe Website: smart-contracts-and-security-assessment-for-fintech-ico.pdf: https://www.softserveinc.com/files/cases/smart-contracts-and-security-assessment-for-fintech-ico.pdf',\n",
       " 'SoftServe Website: leading-coin-offering-asian-fintech-startup.pdf: https://www.softserveinc.com/files/cases/leading-coin-offering-asian-fintech-startup.pdf',\n",
       " 'SoftServe Website: smart-contracts-and-security-assessment-for-fintech-ico.pdf: https://www.softserveinc.com/files/cases/smart-contracts-and-security-assessment-for-fintech-ico.pdf']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_formatted, results, content = search_idx(\n",
    "    search_query, data_sources, top=10\n",
    ")\n",
    "results = [res[\"url\"] + \": \" + res[\"content\"] for res in results_formatted]\n",
    "content = \"\\n\".join(results)\n",
    "\n",
    "[\n",
    "    str(res[\"storage\"]) + \": \"\n",
    "    + str(res[\"title\"]) + \": \" + str(res[\"url\"])\n",
    "    for res in results_formatted\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response_prompt_template = \"\"\"\n",
    "You're an assistant for SoftServe employees, answering their corporate inquiries.\n",
    "\n",
    "In addition to the User Question, you receive a list of the most relevant Sources that may contain the answer. Each source has a URL followed by a colon and the actual source information. DO NOT include a list of Sources in your response.\n",
    "\n",
    "Rules:\n",
    "- Refer to SoftServe using terms like \"we\", \"us\", \"SoftServe\", or \"SoftServe team\". DO NOT use \"they\".\n",
    "- DO NOT include any SoftServe employee personal information in your response or citations (e.g., names, titles, email addresses). If asked, reply with \"I'm not allowed to share that information.\"\n",
    "- You can mention names and titles if the information is from ((https://en.wikipedia.org/wiki/SoftServe)).\n",
    "- Encourage precise queries for better answers. If necessary, request clarification.\n",
    "- For data tables, use HTML format. No Markdown.\n",
    "- Only use provided Sources for answers. If unsure, say \"I don't know.\".\n",
    "- Use as much relevant information from ALL provided Sources as possible. But DO NOT include any irrelevant information from Sources.\n",
    "- Cite each source separately with a full URL in double parentheses, e.g., ((https://softserveinc.com/items/60a77281)), ((source: https://softserveinc.highspot.com/items/64ce04cfc77183892e5ac400))\n",
    "- DO NOT include any additional text in parentheses.\n",
    "- DO NOT combine multiple citations in parentheses.\n",
    "- DO NOT combine multiple Sources in one citation.\n",
    "- Always cite with the full source URL. DO NOT use abbreviations or incomplete URLs.\n",
    "- DO NOT modify the original URLs when citing.\n",
    "Examples of citations:\n",
    "Correct: ((https://www.softserveinc.com/items/60a772914))\n",
    "Wrong: ((softserveinc)), ((demand-analytics.pdf)), ((source)), ((source: https://softserveinc.highspot.com/items/64ce04cfc77183892e5ac400)), (source: ((https://softserveinc.highspot.com/items/6374afff5eca98c3bc16cf94))), ((https://softserveinc.highspot.com/items/60356d674cfd1a15042411ba), (https://www.softserveinc.com/resources/essential-data-engineering)).\n",
    "\n",
    "{follow_up_questions_prompt}\n",
    "{injected_prompt}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response_prompt = ai_response_prompt_template.format(\n",
    "    injected_prompt=\"\",\n",
    "    follow_up_questions_prompt=\"\",\n",
    ")\n",
    "\n",
    "ai_response_messages = get_messages_from_history(\n",
    "    ai_response_prompt,\n",
    "    gpt4_model,\n",
    "    history,\n",
    "    \"User Question:\\n\" + original_user_question + \"\\n\\nSources:\\n\" + content,\n",
    "    max_tokens=gpt4_token_limit - len(ai_response_prompt),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response_completion = await openai.ChatCompletion.acreate(\n",
    "    deployment_id=gpt4_deployment,\n",
    "    model=gpt4_model,\n",
    "    messages=ai_response_messages,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "ai_response = ai_response_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftServe has been involved in several blockchain case studies, focusing on different aspects of this technology.\n",
      "\n",
      "One case study involved building a blockchain-based exchange platform for invoice trading for a tech company based in Asia. The platform was designed to provide easy access to liquidity, accelerate interactions, and improve the world's trading infrastructure. It was built using blockchain technology, which guarantees fast settlement speed, asset tokenization, transparent rules of trade, and provides an asset management infrastructure to all ecosystem participants ((https://www.softserveinc.com/files/cases/building-a-blockchain-based-exchange-platform-invoice-trading-case.pdf)).\n",
      "\n",
      "Another case study was a project for Carnegie Mellon, merging IoT and blockchain for a smart report system. The system was designed to manage data collected through IoT devices and stored securely using blockchain. The data could be used to create valuable reports for decision-making ((https://www.softserveinc.com/files/cases/merging-iot-blockchain-smart-report-system.pdf)).\n",
      "\n",
      "SoftServe also led an initial coin offering for an Asian fintech startup. The client wanted to run the ICO to generate more investments in the business, increase engagement with the platform, and provide more liquidity for investors with future exchange operations. The project involved strategic and technical consulting, creating smart contracts for crowdsale, and conducting a security assessment ((https://www.softserveinc.com/files/cases/leading-coin-offering-asian-fintech-startup.pdf)).\n",
      "\n",
      "In another case, SoftServe worked with a young technology startup from Southeast Asia to develop a platform for invoice trading based on blockchain technology. The client wanted to run an ICO to grow investments in the business, increase engagement with the platform, and provide more liquidity for investors with future operations at the exchange ((https://www.softserveinc.com/files/cases/smart-contracts-and-security-assessment-for-fintech-ico.pdf)). \n",
      "\n",
      "These case studies demonstrate SoftServe's expertise in leveraging blockchain technology to create innovative solutions for various business challenges.\n"
     ]
    }
   ],
   "source": [
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_val_prompt = \"\"\"\n",
    "You're an assistant for SoftServe employees, answering their corporate inquiries. \n",
    "\n",
    "You need to validate your past Response and ensure it complies with the Rules below. In addition to the Response, you receive a list of Source URLs used to generate the Response. DO NOT include a list of Source URLs in the new Response.\n",
    "\n",
    "If the Response doesn't comply with the rules, rewrite ONLY the corresponding parts to meet the requirements and return the new Response. Make ONLY critical changes. If the Response does comply with the rules, return it without changes. DO NOT include any text before or after the new Response. \n",
    "\n",
    "Rules:\n",
    "- Refer to SoftServe using terms like \"we\", \"us\", \"SoftServe\", or \"SoftServe team\". DO NOT use \"they\".\n",
    "- Remove any SoftServe employee personal information from the new Response or citations (e.g., names, titles, email addresses).\n",
    "- You can mention names and titles if the information is from ((https://en.wikipedia.org/wiki/SoftServe)).\n",
    "- For data tables, use HTML format. No Markdown.\n",
    "- Cite each source separately with a full URL in double parentheses, e.g., ((https://softserveinc.com/items/60a77281)), ((source: https://softserveinc.highspot.com/items/64ce04cfc77183892e5ac400))\n",
    "- DO NOT include any additional text in parentheses.\n",
    "- DO NOT combine multiple citations in parentheses.\n",
    "- DO NOT combine multiple sources in one citation.\n",
    "- Always cite with the full source URL. DO NOT use abbreviations or incomplete URLs.\n",
    "- DO NOT modify the original URLs when citing.\n",
    "Examples of citations:\n",
    "Correct: ((https://www.softserveinc.com/items/60a772914))\n",
    "Wrong: ((softserveinc)), ((demand-analytics.pdf)), ((source)), ((source: https://softserveinc.highspot.com/items/64ce04cfc77183892e5ac400)), (source: ((https://softserveinc.highspot.com/items/6374afff5eca98c3bc16cf94))), ((https://softserveinc.highspot.com/items/60356d674cfd1a15042411ba), (https://www.softserveinc.com/resources/essential-data-engineering)).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_val_messages = get_messages_from_history(\n",
    "    response_val_prompt,\n",
    "    chatgpt_model,\n",
    "    [],\n",
    "    \"Response:\\n\"\n",
    "    + ai_response\n",
    "    + \"\\n\\n\\Source URLs:\\n\"\n",
    "    + \"\\n\".join([res[\"url\"] for res in results_formatted]),\n",
    "    max_tokens=chatgpt_token_limit - len(response_val_prompt),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_val_completion = await openai.ChatCompletion.acreate(\n",
    "    deployment_id=chatgpt_deployment,\n",
    "    model=chatgpt_model,\n",
    "    messages=response_val_messages,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    ")\n",
    "validated_response = response_val_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
